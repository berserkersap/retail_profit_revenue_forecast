{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Profit & Revenue Forecasting Demo\n",
    "\n",
    "This notebook demonstrates the retail forecasting system with comprehensive model comparison and interpretability analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our forecasting pipeline\n",
    "from main import RetailForecastPipeline\n",
    "from src.data.data_processor import DataProcessor\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the Forecasting Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = RetailForecastPipeline(\"../config/config.yaml\")\n",
    "\n",
    "print(\"Pipeline initialized successfully!\")\n",
    "print(f\"Available models: {pipeline.model_factory.get_available_models()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Sample Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data (since we don't have actual data file)\n",
    "data_processor = DataProcessor(pipeline.config)\n",
    "sample_data = data_processor._create_sample_data()\n",
    "\n",
    "print(f\"Sample data shape: {sample_data.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(sample_data.head())\n",
    "\n",
    "print(\"\\nData summary:\")\n",
    "print(sample_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Revenue over time\n",
    "daily_revenue = sample_data.groupby('date')['revenue'].sum()\n",
    "axes[0,0].plot(daily_revenue.index, daily_revenue.values)\n",
    "axes[0,0].set_title('Daily Revenue Over Time')\n",
    "axes[0,0].set_ylabel('Revenue')\n",
    "\n",
    "# Profit by category\n",
    "category_profit = sample_data.groupby('product_category')['profit'].mean()\n",
    "axes[0,1].bar(category_profit.index, category_profit.values)\n",
    "axes[0,1].set_title('Average Profit by Category')\n",
    "axes[0,1].set_ylabel('Profit')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Revenue by region\n",
    "region_revenue = sample_data.groupby('region')['revenue'].sum()\n",
    "axes[1,0].pie(region_revenue.values, labels=region_revenue.index, autopct='%1.1f%%')\n",
    "axes[1,0].set_title('Revenue Distribution by Region')\n",
    "\n",
    "# Promotion impact\n",
    "promo_impact = sample_data.groupby('promotion')[['profit', 'revenue']].mean()\n",
    "promo_impact.plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('Promotion Impact')\n",
    "axes[1,1].set_xlabel('Promotion (0=No, 1=Yes)')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for profit forecasting\n",
    "profit_data = pipeline.data_processor.preprocess_for_profit_forecast(sample_data, 'profit')\n",
    "print(f\"Processed profit data shape: {profit_data.shape}\")\n",
    "print(f\"Features: {list(profit_data.columns)}\")\n",
    "\n",
    "# Show feature statistics\n",
    "print(\"\\nFeature statistics:\")\n",
    "numeric_cols = profit_data.select_dtypes(include=[np.number]).columns\n",
    "print(profit_data[numeric_cols].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run profit forecasting\n",
    "print(\"Training models for profit forecasting...\")\n",
    "profit_results = pipeline.run_profit_forecast(\"dummy_path\", 'profit')\n",
    "\n",
    "# Display comparison results\n",
    "comparison = profit_results['comparison']\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics = ['rmse', 'mae', 'r2', 'mape']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    if metric in comparison.columns:\n",
    "        bars = ax.bar(comparison['model'], comparison[metric])\n",
    "        ax.set_title(f'{metric.upper()} by Model')\n",
    "        ax.set_ylabel(metric.upper())\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretability Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display interpretability results\n",
    "interpretability = profit_results['interpretability']\n",
    "\n",
    "for model_name, results in interpretability.items():\n",
    "    if 'error' in results:\n",
    "        print(f\"\\n{model_name.upper()} - Error: {results['error']}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n=== {model_name.upper()} INTERPRETABILITY ===\")\n",
    "    \n",
    "    # SHAP importance\n",
    "    if 'shap_importance' in results:\n",
    "        shap_imp = results['shap_importance']\n",
    "        print(f\"\\nTop 5 Most Important Features (SHAP):\")\n",
    "        print(shap_imp.head().to_string(index=False))\n",
    "    \n",
    "    # Profit vs Loss drivers\n",
    "    if 'profit_loss_drivers' in results and 'feature_impact' in results['profit_loss_drivers']:\n",
    "        drivers = results['profit_loss_drivers']\n",
    "        if 'top_profit_drivers' in drivers:\n",
    "            print(f\"\\nTop Profit Drivers:\")\n",
    "            print(drivers['top_profit_drivers'][['feature', 'difference']].head().to_string(index=False))\n",
    "        \n",
    "        if 'top_loss_drivers' in drivers:\n",
    "            print(f\"\\nTop Loss Drivers:\")\n",
    "            print(drivers['top_loss_drivers'][['feature', 'difference']].head().to_string(index=False))\n",
    "    \n",
    "    # Seasonal impact\n",
    "    if 'seasonal_impact' in results and results['seasonal_impact']:\n",
    "        print(f\"\\nSeasonal Impact:\")\n",
    "        for feature, impact in list(results['seasonal_impact'].items())[:3]:\n",
    "            print(f\"  {feature}: Mean Impact = {impact['mean_impact']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for tree-based models\n",
    "tree_models = ['xgboost', 'lightgbm']\n",
    "fig, axes = plt.subplots(1, len(tree_models), figsize=(15, 8))\n",
    "\n",
    "for i, model_name in enumerate(tree_models):\n",
    "    if model_name in interpretability:\n",
    "        results = interpretability[model_name]\n",
    "        if 'shap_importance' in results:\n",
    "            importance = results['shap_importance'].head(10)\n",
    "            \n",
    "            ax = axes[i] if len(tree_models) > 1 else axes\n",
    "            bars = ax.barh(range(len(importance)), importance['shap_importance'])\n",
    "            ax.set_yticks(range(len(importance)))\n",
    "            ax.set_yticklabels(importance['feature'])\n",
    "            ax.set_xlabel('SHAP Importance')\n",
    "            ax.set_title(f'{model_name.upper()} Feature Importance')\n",
    "            ax.invert_yaxis()\n",
    "            \n",
    "            # Add value labels\n",
    "            for j, bar in enumerate(bars):\n",
    "                width = bar.get_width()\n",
    "                ax.text(width + width*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                       f'{width:.3f}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual vs predicted values\n",
    "predictions = profit_results['predictions']\n",
    "\n",
    "# For demonstration, we'll use the test data from the last model training\n",
    "# In practice, you'd want to pass the actual test values\n",
    "test_data = pipeline.data_processor.split_data(profit_data)[2]  # Get test split\n",
    "actual_values = test_data['profit'].values\n",
    "\n",
    "# Plot predictions vs actual\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model_name, preds) in enumerate(predictions.items()):\n",
    "    if i >= 4:  # Only plot first 4 models\n",
    "        break\n",
    "        \n",
    "    # Ensure same length\n",
    "    min_length = min(len(preds), len(actual_values))\n",
    "    preds_trimmed = preds[:min_length]\n",
    "    actual_trimmed = actual_values[:min_length]\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[i].scatter(actual_trimmed, preds_trimmed, alpha=0.6)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(np.min(actual_trimmed), np.min(preds_trimmed))\n",
    "    max_val = max(np.max(actual_trimmed), np.max(preds_trimmed))\n",
    "    axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    \n",
    "    axes[i].set_title(f'{model_name.upper()}')\n",
    "    axes[i].set_xlabel('Actual')\n",
    "    axes[i].set_ylabel('Predicted')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate and display R¬≤\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(actual_trimmed, preds_trimmed)\n",
    "    axes[i].text(0.05, 0.95, f'R¬≤ = {r2:.3f}', transform=axes[i].transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Series Forecast Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot of predictions\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot actual values\n",
    "x_values = range(len(actual_values))\n",
    "plt.plot(x_values, actual_values, label='Actual', linewidth=2, color='black')\n",
    "\n",
    "# Plot predictions for each model\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "for (model_name, preds), color in zip(predictions.items(), colors):\n",
    "    # Ensure same length\n",
    "    min_length = min(len(preds), len(actual_values))\n",
    "    preds_trimmed = preds[:min_length]\n",
    "    \n",
    "    plt.plot(range(min_length), preds_trimmed, \n",
    "             label=f'{model_name.title()} Prediction', \n",
    "             linewidth=1.5, alpha=0.8, color=color)\n",
    "\n",
    "plt.title('Model Predictions vs Actual Values Over Time', fontsize=14)\n",
    "plt.xlabel('Time Period')\n",
    "plt.ylabel('Profit')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Business Insights Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== BUSINESS INSIGHTS SUMMARY ===\")\n",
    "print()\n",
    "\n",
    "# Best performing model\n",
    "best_model = comparison.loc[comparison['rmse'].idxmin(), 'model']\n",
    "best_rmse = comparison.loc[comparison['rmse'].idxmin(), 'rmse']\n",
    "print(f\"üèÜ BEST PERFORMING MODEL: {best_model.upper()}\")\n",
    "print(f\"   RMSE: {best_rmse:.3f}\")\n",
    "print()\n",
    "\n",
    "# Key findings from interpretability\n",
    "print(\"üìä KEY FINDINGS:\")\n",
    "print()\n",
    "\n",
    "if best_model in interpretability:\n",
    "    best_results = interpretability[best_model]\n",
    "    \n",
    "    if 'shap_importance' in best_results:\n",
    "        top_feature = best_results['shap_importance'].iloc[0]\n",
    "        print(f\"üí° Most Important Feature: {top_feature['feature']}\")\n",
    "        print(f\"   Impact Score: {top_feature['shap_importance']:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    if 'profit_loss_drivers' in best_results:\n",
    "        drivers = best_results['profit_loss_drivers']\n",
    "        if 'top_profit_drivers' in drivers and len(drivers['top_profit_drivers']) > 0:\n",
    "            top_profit_driver = drivers['top_profit_drivers'].iloc[0]\n",
    "            print(f\"üìà Top Profit Driver: {top_profit_driver['feature']}\")\n",
    "            print(f\"   Impact: {top_profit_driver['difference']:.3f}\")\n",
    "            print()\n",
    "            \n",
    "        if 'top_loss_drivers' in drivers and len(drivers['top_loss_drivers']) > 0:\n",
    "            top_loss_driver = drivers['top_loss_drivers'].iloc[0]\n",
    "            print(f\"üìâ Top Loss Driver: {top_loss_driver['feature']}\")\n",
    "            print(f\"   Impact: {top_loss_driver['difference']:.3f}\")\n",
    "            print()\n",
    "\n",
    "# Model performance summary\n",
    "print(\"üéØ MODEL PERFORMANCE RANKING:\")\n",
    "for i, row in comparison.iterrows():\n",
    "    print(f\"   {row['rank']}. {row['model'].upper()} - RMSE: {row['rmse']:.3f}\")\n",
    "print()\n",
    "\n",
    "# Business recommendations\n",
    "print(\"üíº BUSINESS RECOMMENDATIONS:\")\n",
    "print(\"   1. Deploy the best performing model in production\")\n",
    "print(\"   2. Focus on top profit drivers to maximize returns\")\n",
    "print(\"   3. Address top loss drivers to minimize losses\")\n",
    "print(\"   4. Monitor seasonal patterns for inventory planning\")\n",
    "print(\"   5. Optimize promotional strategies based on impact analysis\")\n",
    "print(\"   6. Consider regional differences in forecasting models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "import os\n",
    "os.makedirs('notebook_outputs', exist_ok=True)\n",
    "\n",
    "# Save model comparison results\n",
    "comparison.to_csv('notebook_outputs/model_comparison.csv', index=False)\n",
    "print(\"‚úÖ Model comparison results saved to 'notebook_outputs/model_comparison.csv'\")\n",
    "\n",
    "# Save feature importance for best model\n",
    "if best_model in interpretability and 'shap_importance' in interpretability[best_model]:\n",
    "    interpretability[best_model]['shap_importance'].to_csv(\n",
    "        f'notebook_outputs/{best_model}_feature_importance.csv', index=False)\n",
    "    print(f\"‚úÖ Feature importance saved to 'notebook_outputs/{best_model}_feature_importance.csv'\")\n",
    "\n",
    "# Generate final visualizations\n",
    "try:\n",
    "    pipeline.generate_visualizations(profit_results, \"notebook_outputs/\")\n",
    "    print(\"‚úÖ Visualizations saved to 'notebook_outputs/' directory\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not generate all visualizations: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Analysis complete! Check the 'notebook_outputs' directory for all results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}